{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c5e974",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression is a popular classification technique due to its simplicity, interpretability and performance. \n",
    "\n",
    "The goal of this project is to get insight into the following topics:\n",
    "- Being able to train a logistic regression model.\n",
    "- Performing hyperparameter tuning.\n",
    "- Testing and evaluation of the trained model. Being able to assess the model by means of different classification metrics: accuracy, recall, precision, f1-score, ROC.\n",
    "- Knowing how to deal with underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8771fc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SNtolo\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns # fancy plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf9f2e",
   "metadata": {},
   "source": [
    "## 1. Wine classification\n",
    "\n",
    "The dataset *wine_data.csv* consists of chemical parameters of several different wines. Your task is to train a logistic regression model being able to classify these wines by cultivar (wine farmer). In other words can you predict who produced a specific wine?\n",
    "\n",
    "The column cultivar contains the target values (what you want to predict). For example 1 stands for cultivar 1.\n",
    "\n",
    "### Reading the dataset and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de5b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs Alcohol</th>\n",
       "      <th>MalicAcid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>AlcalinityOfAsh</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>TotalPhenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>NonflavanoidsPhenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>ColorIntensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>23.6</td>\n",
       "      <td>70</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.21</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.51</td>\n",
       "      <td>24.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.72</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.92</td>\n",
       "      <td>19.6</td>\n",
       "      <td>78</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.68</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.48</td>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.11</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.70</td>\n",
       "      <td>15.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.18</td>\n",
       "      <td>502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.38</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.57</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.25</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.69</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.7</td>\n",
       "      <td>80</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.06</td>\n",
       "      <td>495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.77</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.98</td>\n",
       "      <td>16.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.12</td>\n",
       "      <td>372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.88</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2.23</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.68</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.33</td>\n",
       "      <td>415</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.5</td>\n",
       "      <td>81</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.27</td>\n",
       "      <td>480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inputs Alcohol  MalicAcid   Ash  AlcalinityOfAsh  Magnesium  TotalPhenols  \\\n",
       "0           12.08       1.33  2.30             23.6         70          2.20   \n",
       "1           12.08       1.13  2.51             24.0         78          2.00   \n",
       "2           12.37       1.17  1.92             19.6         78          2.11   \n",
       "3           13.11       1.01  1.70             15.0         78          2.98   \n",
       "4           12.04       4.30  2.38             22.0         80          2.10   \n",
       "5           12.25       1.73  2.12             19.0         80          1.65   \n",
       "6           12.69       1.53  2.26             20.7         80          1.38   \n",
       "7           12.77       3.43  1.98             16.0         80          1.63   \n",
       "8           13.88       5.04  2.23             20.0         80          0.98   \n",
       "9           12.08       1.83  2.32             18.5         81          1.60   \n",
       "\n",
       "   flavanoids  NonflavanoidsPhenols  Proanthocyanins  ColorIntensity   Hue  \\\n",
       "0        1.59                  0.42             1.38            1.74  1.07   \n",
       "1        1.58                  0.40             1.40            2.20  1.31   \n",
       "2        2.00                  0.27             1.04            4.68  1.12   \n",
       "3        3.18                  0.26             2.28            5.30  1.12   \n",
       "4        1.75                  0.42             1.35            2.60  0.79   \n",
       "5        2.03                  0.37             1.63            3.40  1.00   \n",
       "6        1.46                  0.58             1.62            3.05  0.96   \n",
       "7        1.25                  0.43             0.83            3.40  0.70   \n",
       "8        0.34                  0.40             0.68            4.90  0.58   \n",
       "9        1.50                  0.52             1.64            2.40  1.08   \n",
       "\n",
       "   OD280/OD315  Proline  Cultivar  \n",
       "0         3.21      625         1  \n",
       "1         2.72      630         1  \n",
       "2         3.48      510         1  \n",
       "3         3.18      502         1  \n",
       "4         2.57      580         1  \n",
       "5         3.17      510         1  \n",
       "6         2.06      495         1  \n",
       "7         2.12      372         1  \n",
       "8         1.33      415         2  \n",
       "9         2.27      480         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "dataset = pd.read_csv('wine_data.csv')\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63056ee2",
   "metadata": {},
   "source": [
    "Determine the number of classes. In other words, how many different cultivars are in the dataset? \n",
    "Is the dataset balanced? Balanced means that the distribution of the targets is more or less uniformly distributed.\n",
    "For this dataset balanced means that you have approximately the same number wines for each of the cultivars.\n",
    "You can use the Seaborn countplot for this. https://seaborn.pydata.org/generated/seaborn.countplot.html.\n",
    "The problem with imbalanced datasets is that the model you train can get a preference for the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8026e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Cultivar', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of classes (countplot)\n",
    "sns.countplot(x = 'Cultivar', data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57dbcf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inputs Alcohol</th>\n",
       "      <td>178.0</td>\n",
       "      <td>13.000618</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>11.03</td>\n",
       "      <td>12.3625</td>\n",
       "      <td>13.050</td>\n",
       "      <td>13.6775</td>\n",
       "      <td>14.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MalicAcid</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.6025</td>\n",
       "      <td>1.865</td>\n",
       "      <td>3.0825</td>\n",
       "      <td>5.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ash</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.2100</td>\n",
       "      <td>2.360</td>\n",
       "      <td>2.5575</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlcalinityOfAsh</th>\n",
       "      <td>178.0</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>10.60</td>\n",
       "      <td>17.2000</td>\n",
       "      <td>19.500</td>\n",
       "      <td>21.5000</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magnesium</th>\n",
       "      <td>178.0</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>70.00</td>\n",
       "      <td>88.0000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>107.0000</td>\n",
       "      <td>162.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalPhenols</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.7425</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.8000</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavanoids</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.2050</td>\n",
       "      <td>2.135</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonflavanoidsPhenols</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <td>178.0</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.9500</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ColorIntensity</th>\n",
       "      <td>178.0</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.2200</td>\n",
       "      <td>4.690</td>\n",
       "      <td>6.2000</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hue</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.1200</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OD280/OD315</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.9375</td>\n",
       "      <td>2.780</td>\n",
       "      <td>3.1700</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proline</th>\n",
       "      <td>178.0</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>278.00</td>\n",
       "      <td>500.5000</td>\n",
       "      <td>673.500</td>\n",
       "      <td>985.0000</td>\n",
       "      <td>1680.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cultivar</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.775035</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count        mean         std     min       25%  \\\n",
       "inputs Alcohol        178.0   13.000618    0.811827   11.03   12.3625   \n",
       "MalicAcid             178.0    2.336348    1.117146    0.74    1.6025   \n",
       "Ash                   178.0    2.366517    0.274344    1.36    2.2100   \n",
       "AlcalinityOfAsh       178.0   19.494944    3.339564   10.60   17.2000   \n",
       "Magnesium             178.0   99.741573   14.282484   70.00   88.0000   \n",
       "TotalPhenols          178.0    2.295112    0.625851    0.98    1.7425   \n",
       "flavanoids            178.0    2.029270    0.998859    0.34    1.2050   \n",
       "NonflavanoidsPhenols  178.0    0.361854    0.124453    0.13    0.2700   \n",
       "Proanthocyanins       178.0    1.590899    0.572359    0.41    1.2500   \n",
       "ColorIntensity        178.0    5.058090    2.318286    1.28    3.2200   \n",
       "Hue                   178.0    0.957449    0.228572    0.48    0.7825   \n",
       "OD280/OD315           178.0    2.611685    0.709990    1.27    1.9375   \n",
       "Proline               178.0  746.893258  314.907474  278.00  500.5000   \n",
       "Cultivar              178.0    0.938202    0.775035    0.00    0.0000   \n",
       "\n",
       "                          50%       75%      max  \n",
       "inputs Alcohol         13.050   13.6775    14.83  \n",
       "MalicAcid               1.865    3.0825     5.80  \n",
       "Ash                     2.360    2.5575     3.23  \n",
       "AlcalinityOfAsh        19.500   21.5000    30.00  \n",
       "Magnesium              98.000  107.0000   162.00  \n",
       "TotalPhenols            2.355    2.8000     3.88  \n",
       "flavanoids              2.135    2.8750     5.08  \n",
       "NonflavanoidsPhenols    0.340    0.4375     0.66  \n",
       "Proanthocyanins         1.555    1.9500     3.58  \n",
       "ColorIntensity          4.690    6.2000    13.00  \n",
       "Hue                     0.965    1.1200     1.71  \n",
       "OD280/OD315             2.780    3.1700     4.00  \n",
       "Proline               673.500  985.0000  1680.00  \n",
       "Cultivar                1.000    2.0000     2.00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical analysis of the data\n",
    "dataset.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa041100",
   "metadata": {},
   "source": [
    "### Preprocessing and compiling a training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa3794",
   "metadata": {},
   "source": [
    " Next is to Split into **features and targets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d5ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178,) (178, 13)\n",
      "[[ 12.08   1.33   2.3  ...   1.07   3.21 625.  ]\n",
      " [ 12.08   1.13   2.51 ...   1.31   2.72 630.  ]\n",
      " [ 12.37   1.17   1.92 ...   1.12   3.48 510.  ]\n",
      " ...\n",
      " [ 12.99   1.67   2.6  ...   1.31   3.5  985.  ]\n",
      " [ 12.21   1.19   1.75 ...   1.28   3.07 718.  ]\n",
      " [ 12.47   1.52   2.2  ...   1.16   2.63 937.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting into features and targets\n",
    "y = dataset['Cultivar'].values     #Setting Cultivar as target\n",
    "X = dataset.drop('Cultivar', axis=1).values    # Features\n",
    "print (y.shape, X.shape)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6258f",
   "metadata": {},
   "source": [
    "Create a training set and test set. Make sure that 70 wines end up in the test set.\n",
    "Use the train_test_split function. More information:http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96db0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=70, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ee57e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97fd79",
   "metadata": {},
   "source": [
    "We now going to Scale the dataset by means of a Standardscaler or MinMax scaler. To read more about this, please check the link for documentation \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfac5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with standardscaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef2d0759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5839572  -0.5320314   1.18779565 ... -1.98543036 -1.1051815\n",
      "  -0.2847604 ]\n",
      " [-0.35734832 -1.16704205 -0.53059945 ...  1.11566255  0.66748586\n",
      "  -0.92199434]\n",
      " [-0.43872041 -0.84046515 -1.80071757 ...  0.22292368  0.09136897\n",
      "  -0.58213624]\n",
      " ...\n",
      " [-0.79908251 -0.59553247 -0.6426687  ... -0.40669215 -1.31199269\n",
      "   0.35247353]\n",
      " [-0.84558085  2.2257291   0.590093   ... -1.13967775 -2.12446523\n",
      "  -0.10269356]\n",
      " [ 0.90973133 -0.66810511  1.18779565 ...  1.20963507  0.43113021\n",
      "   1.4600468 ]]\n"
     ]
    }
   ],
   "source": [
    "# Lets see our X_train\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6979d2",
   "metadata": {},
   "source": [
    "To Train a logistic regression classifier please visit the link and it will give you more documentation about this - (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Initially, we can choose C=1.\n",
    "\n",
    "Next is to test the trained model on the test set. We then Compute the confusion matrix, accuracy and generate the classification report to see our findings\n",
    "\n",
    "To Try and improve the accuracy of the model by hyperparameter tuning. Options for hyperparameter tuning are the following:\n",
    "- Find a good C-value\n",
    "- Choose a different solver\n",
    "- Apply L1 or L2 regularization (penalty parameter)\n",
    "- Polynomial expansion of the features: adding higher order features.\n",
    "- In case you have an imbalanced dataset you can use the **class_weight=balanced** parameter. Explain the effect of it in terms of accuracy , recall, precision and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66797b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=2)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5866d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        22\n",
      "           1       1.00      0.96      0.98        24\n",
      "           2       0.96      1.00      0.98        24\n",
      "\n",
      "    accuracy                           0.99        70\n",
      "   macro avg       0.99      0.99      0.99        70\n",
      "weighted avg       0.99      0.99      0.99        70\n",
      "\n",
      "98.57142857142858\n",
      "[[22  0  0]\n",
      " [ 0 23  1]\n",
      " [ 0  0 24]]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test,logreg_pred))\n",
    "logreg_acc = accuracy_score(logreg_pred, y_test)\n",
    "print (logreg_acc*100)\n",
    "print(confusion_matrix(y_test,logreg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934e9c3",
   "metadata": {},
   "source": [
    "**C = 100 gives us 97% which is not much different** \n",
    "\n",
    "**C = 2000 gives us 96%, decreased by 1**\n",
    "\n",
    "**C = 2 gives us 99%.. much better!**\n",
    "\n",
    "**We can conclude than when C value goes up, our prediction goes down**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d9647",
   "metadata": {},
   "source": [
    "### Predictions with the trained model\n",
    "\n",
    "Predict the cultivar for the following wine:\n",
    "\n",
    "inputs Alcohol: 13.52 - \n",
    "MalicAcid: 2.05 - \n",
    "Ash: 2.20 - \n",
    "AlcalinityOfAsh: 17.3 - \n",
    "Magnesium: 120 - \n",
    "TotalPhenols: 2.60 - \n",
    "flavanoids: 3.52 - \n",
    "NonflavanoidsPhenols: 0.30 - \n",
    "Proanthocyanins: 2.28 - \n",
    "ColorIntensity: 7.80 - \n",
    "Hue: 0.77 - \n",
    "OD280/OD315: 2.90 - \n",
    "Proline: 862\n",
    "\n",
    "\n",
    "But what is the confidence of our model? Let's use the predict_proba() function to compute this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00caf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d3075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857142857142858"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339767f",
   "metadata": {},
   "source": [
    "**This gives us 98%!! not bad but one can say there is a bit of biased in the model. Also, there might be overfitting maybe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdbff21",
   "metadata": {},
   "source": [
    "Below I will perform some resampling using **Synthetic Minority Oversampling Technique - SMOTE** and see if we can accuractly balance this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26168c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634a7ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 47, 47], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using numpy to perform the imbalance count\n",
    "np.bincount(y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ebad6",
   "metadata": {},
   "source": [
    "**Now we can see that the dataset is accuractly blanced using SMOTE. I will test the model using the resampled dataset for training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a72ca",
   "metadata": {},
   "source": [
    "Now I am going to train on a new resampled data using X_res and y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2366eb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LogisticRegression()\n",
    "lr2.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abfe2c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.score(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fab5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "recall_score_dict = {}\n",
    "acc_score_dict = {}\n",
    "precision_score_dict = {}\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataset[\"Cultivar\"] = le.fit_transform(dataset[\"Cultivar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f198fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logreg1 = LogisticRegression(random_state=0)\n",
    "\n",
    "#Fitting the training data\n",
    "logreg1.fit(X_train_res, y_train_res)\n",
    "\n",
    "#Predicting on test\n",
    "y_pred1=logreg1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595c8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using inputs:\n",
    "\n",
    "wine = np.array([[13.52,2.05,2.20,17.3,120,2.60,3.52,0.30,2.28,7.80,0.77,2.90,862]])\n",
    "wine = scaler.transform(wine)\n",
    "\n",
    "winefarmer = logreg.predict(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bc14421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(winefarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944e803",
   "metadata": {},
   "source": [
    "**It appears that the model thinks the wine belongs to wine farmer 0 with ~98%**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04ae8632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86393193  0.17758964  0.505721   -0.58450971  0.13960935  0.27161449\n",
      "   0.66474254 -0.37873515  0.04601904  0.34022765  0.13060706  0.74995176\n",
      "   1.1122689 ]\n",
      " [-1.12694981 -0.54811899 -0.74249081  0.2815334  -0.15389945  0.13298779\n",
      "   0.08025884  0.3005742   0.45539586 -1.24386011  0.66755552  0.03562051\n",
      "  -0.89589149]\n",
      " [ 0.26301787  0.37052935  0.2367698   0.30297631  0.0142901  -0.40460228\n",
      "  -0.74500138  0.07816095 -0.50141489  0.90363246 -0.79816258 -0.78557227\n",
      "  -0.21637741]]\n",
      "[ 0.42577416  1.10207206 -1.52784622]\n"
     ]
    }
   ],
   "source": [
    "# Interpretation of the coefficients and intercepts\n",
    "print (logreg.coef_)\n",
    "print (logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07006f",
   "metadata": {},
   "source": [
    "... to be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8b423",
   "metadata": {},
   "source": [
    "### Author: Siya the LoneWolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd6f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
